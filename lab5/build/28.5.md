# Анализ решения задачи нелинейной регрессии

## Описание задачи

Задача 28.5 представляет собой **нелинейную регрессию** — одну из фундаментальных задач машинного обучения и статистического анализа. Цель состоит в поиске оптимальных параметров сложных нелинейных функций для наилучшей аппроксимации экспериментальных данных.

### Математическая формулировка:

**Одномерная модель:**
```
f(x) = arctan(p₁x) × exp(-(x - p₂)) + cos²(x) × sin(1/(1 + exp(-p₃x)))
```

**Многомерная модель:**
```
f(x,y) = sin(p₁x²y) × exp(p₂(x - y²))
```

**Функции потерь:**
1. **L2-норма (квадратичная):** `Σ(f_pred - f_true)²`
2. **Функция потерь Хубера:** Комбинация квадратичной и линейной потерь

## Используемые алгоритмы оптимизации

### BFGS (Broyden-Fletcher-Goldfarb-Shanno)

**Класс алгоритма:** Квази-ньютоновский метод оптимизации

#### Принцип работы:
1. **Аппроксимация матрицы Гессе** без вычисления вторых производных
2. **Использование градиентов** для построения направления поиска
3. **Итеративное обновление** аппроксимации матрицы Гессе

#### Преимущества BFGS:
- **Суперлинейная сходимость** (быстрее градиентного спуска)
- **Не требует вычисления матрицы Гессе** (экономия вычислений)
- **Хорошая устойчивость** к локальным особенностям функции
- **Автоматическая адаптация** размера шага

#### Почему BFGS эффективен для данной задачи:
- **Гладкие нелинейные функции**: arctan, exp, sin, cos
- **Непрерывные градиенты** во всей области определения
- **Умеренная размерность**: 3 параметра (1D) и 2 параметра (2D)

## Детальный анализ результатов

### Одномерная регрессия - выдающееся качество

#### Численные характеристики:
- **R² = 0.9887** — объясняет 98.87% дисперсии данных
- **RMSE = 0.291** — средняя ошибка составляет ~0.29
- **Максимальная ошибка = 0.598** — все отклонения контролируемы
- **21 итерация** для сходимости

#### Качественная оценка:
```
Отличное качество аппроксимации:
- Высокий коэффициент детерминации (R² ≈ 99%)
- Низкие остаточные ошибки
- Быстрая сходимость алгоритма
```

### Многомерная регрессия - сложности с моделью

#### Численные характеристики:
- **R² = -0.047** — модель хуже простого среднего значения
- **RMSE = 0.355** — относительно высокая ошибка
- **Максимальная ошибка = 1.39** — значительные отклонения
- **11 итераций** для сходимости

#### Проблемы модели:
```
Отрицательный R² указывает на:
- Неподходящую структуру модели для данных
- Возможное переобучение или недообучение
- Необходимость пересмотра архитектуры функции
```

## Сравнение функций потерь

### L2-норма vs. Функция Хубера

| Характеристика | L2-норма | Функция Хубера |
|----------------|----------|----------------|
| **Чувствительность к выбросам** | Высокая | Низкая |
| **Математическая форма** | Квадратичная | Кусочно-линейная |
| **Robustness** | Низкая | Высокая |
| **Вычислительная сложность** | Простая | Умеренная |

### Результаты для одномерной модели:
- **L2**: SSE = 8.53, итераций = 21
- **Хубер**: Loss = 4.26, итераций = 19

**Вывод:** Функция Хубера показывает лучшие результаты и быстрее сходится.

### Эффект параметра δ в функции Хубера:

| δ | Потери Хубера (1D) | Потери Хубера (2D) |
|---|--------------------|--------------------|
| 0.5 | 4.256 | 93.202 |
| 1.0 | 4.264 | 105.482 |
| 1.5 | 4.264 | 105.853 |
| 2.0 | 4.264 | 105.853 |

**Наблюдение:** При δ ≥ 1.0 параметры стабилизируются.

## Наихудшие способы решения

### 1. **Случайный поиск (Monte Carlo)**

```python
def worst_random_search():
    best_loss = float('inf')
    best_params = None
    
    for _ in range(1000000):  # Миллион случайных попыток
        p = np.random.uniform(-10, 10, 3)  # Случайные параметры
        loss = sse_1(p)
        if loss < best_loss:
            best_loss = loss
            best_params = p
    
    return best_params
```

**Проблемы:**
- **Экспоненциальный рост времени** с размерностью
- **Нет гарантии сходимости** к оптимуму
- **Игнорирование структуры функции**
- **Ожидаемая эффективность:** R² < 0.5, время >> часов

### 2. **Градиентный спуск с фиксированным шагом**

```python
def naive_gradient_descent():
    lr = 0.001  # Фиксированный малый шаг
    p = np.random.randn(3)
    
    for i in range(100000):  # Много итераций
        grad = numerical_gradient(sse_1, p)
        p = p - lr * grad
        
        # Проблемы:
        # - Может застрять в локальном минимуме
        # - Медленная сходимость
        # - Чувствительность к выбору lr
```

**Недостатки:**
- **Линейная сходимость** (медленно)
- **Требует ручной настройки** learning rate
- **Чувствительность к инициализации**

### 3. **Переобучение с избыточным количеством параметров**

```python
def overfitting_approach():
    # Добавление лишних параметров
    def complex_model(p, x):
        return (sum(p[i] * x**i for i in range(20)) + 
                sum(p[20+i] * np.sin(i*x) for i in range(10)) +
                sum(p[30+i] * np.cos(i*x) for i in range(10)))
    
    # 40 параметров для 101 точки
    # Результат: идеальная подгонка к данным, но нет обобщения
```

**Проблемы:**
- **Curse of dimensionality**
- **Потеря обобщающей способности**
- **Вычислительная сложность O(n³)**

## Лучшие способы решения

### 1. **Текущий подход: BFGS с правильной инициализацией** ⭐

**Преимущества текущего решения:**
- **Быстрая сходимость:** 11-21 итерация
- **Высокая точность:** R² = 98.87% для 1D
- **Устойчивость:** успешная оптимизация в обоих случаях

**Рекомендуемые улучшения:**
```python
def improved_bfgs():
    # Множественные запуски с разной инициализацией
    best_result = None
    best_loss = float('inf')
    
    for seed in range(10):
        np.random.seed(seed)
        p0 = np.random.uniform(-2, 2, n_params)
        
        result = minimize(loss_function, p0, method='BFGS')
        if result.success and result.fun < best_loss:
            best_loss = result.fun
            best_result = result
    
    return best_result
```

### 2. **Trust Region методы**

```python
from scipy.optimize import minimize

def trust_region_approach():
    result = minimize(sse_1, p0, method='trust-ncg', 
                     jac=analytical_gradient,
                     hess=analytical_hessian)
    return result
```

**Преимущества:**
- **Глобальная сходимость**
- **Устойчивость к плохой инициализации**
- **Лучше обрабатывает ill-conditioned проблемы**

### 3. **Левенберг-Марквардт (специально для регрессии)**

```python
from scipy.optimize import least_squares

def levenberg_marquardt():
    def residuals(p):
        return model_fun_1(p, data_1['X']) - data_1['F']
    
    result = least_squares(residuals, p0, method='lm')
    return result
```

**Преимущества для регрессии:**
- **Оптимизирован для задач наименьших квадратов**
- **Автоматическое демпфирование**
- **Робастность к локальным минимумам**

### 4. **Ансамблевые методы**

```python
def ensemble_optimization():
    methods = ['BFGS', 'L-BFGS-B', 'trust-ncg', 'Powell']
    results = []
    
    for method in methods:
        for _ in range(5):  # 5 запусков каждого метода
            p0 = generate_smart_initial_guess()
            result = minimize(loss_function, p0, method=method)
            if result.success:
                results.append(result)
    
    # Выбрать лучший результат
    best = min(results, key=lambda r: r.fun)
    return best
```

### 5. **Байесовская оптимизация для сложных случаев**

```python
from skopt import gp_minimize

def bayesian_optimization():
    # Для случаев с дорогими вычислениями функции
    # Использует Gaussian Process для моделирования функции потерь
    
    def objective(params):
        return sse_1(np.array(params))
    
    bounds = [(-5.0, 5.0)] * n_params
    result = gp_minimize(objective, bounds, n_calls=100)
    return result
```

## Анализ проблем многомерной модели

### Диагностика отрицательного R²:

#### Возможные причины:
1. **Неподходящая архитектура модели**
2. **Недостаток данных** в некоторых областях
3. **Высокий уровень шума** в данных
4. **Неправильная инициализация** параметров

#### Рекомендуемые решения:

### 1. **Анализ остатков:**
```python
def analyze_residuals():
    residuals = model_pred - data_true
    
    # Проверка на паттерны в остатках
    plt.scatter(model_pred, residuals)
    plt.xlabel('Predicted values')
    plt.ylabel('Residuals')
    
    # Тест на нормальность остатков
    from scipy.stats import shapiro
    stat, p_value = shapiro(residuals)
```

### 2. **Альтернативные архитектуры:**
```python
# Более простая модель
def simple_model(p, x, y):
    return p[0] * x + p[1] * y + p[2] * x * y + p[3]

# Полиномиальная модель
def polynomial_model(p, x, y):
    return (p[0] + p[1]*x + p[2]*y + p[3]*x**2 + 
            p[4]*y**2 + p[5]*x*y + p[6]*x**3 + p[7]*y**3)

# Радиально-базисная модель
def rbf_model(p, x, y):
    centers = [(0, 0), (1, 1), (-1, -1), (1, -1), (-1, 1)]
    result = p[0]
    for i, (cx, cy) in enumerate(centers):
        r = np.sqrt((x - cx)**2 + (y - cy)**2)
        result += p[i+1] * np.exp(-p[i+6] * r**2)
    return result
```

### 3. **Регуляризация:**
```python
def regularized_loss(p, alpha=0.01):
    base_loss = sse_many(p)
    l2_penalty = alpha * np.sum(p**2)
    return base_loss + l2_penalty
```

## Практические рекомендации

### Для улучшения текущего решения:

#### 1. **Одномерная модель (уже хорошая):**
```python
# Добавить доверительные интервалы
def confidence_intervals():
    # Bootstrap для оценки неопределенности параметров
    bootstrap_params = []
    for _ in range(1000):
        indices = np.random.choice(len(data_1), len(data_1), replace=True)
        bootstrap_data = data_1.iloc[indices]
        # Переобучить модель на bootstrap выборке
        result = minimize(sse_bootstrap, p0, method='BFGS')
        if result.success:
            bootstrap_params.append(result.x)
    
    # Вычислить 95% доверительные интервалы
    ci_lower = np.percentile(bootstrap_params, 2.5, axis=0)
    ci_upper = np.percentile(bootstrap_params, 97.5, axis=0)
```

#### 2. **Многомерная модель (требует улучшения):**
```python
def model_selection():
    models = [
        lambda p, x, y: p[0] * np.sin(p[1] * x * y),
        lambda p, x, y: p[0] * x + p[1] * y + p[2] * x * y,
        lambda p, x, y: p[0] * np.exp(p[1] * (x**2 + y**2)),
        # ... другие варианты
    ]
    
    aic_scores = []
    for model in models:
        result = fit_model(model)
        k = len(result.x)  # количество параметров
        n = len(data_many)  # количество наблюдений
        aic = 2 * k + n * np.log(result.fun / n)
        aic_scores.append(aic)
    
    best_model_idx = np.argmin(aic_scores)
    return models[best_model_idx]
```

### 3. **Cross-validation:**
```python
def cross_validation(model_func, k_folds=5):
    from sklearn.model_selection import KFold
    
    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
    cv_scores = []
    
    for train_idx, val_idx in kf.split(data_many):
        train_data = data_many.iloc[train_idx]
        val_data = data_many.iloc[val_idx]
        
        # Обучение на train, тестирование на val
        result = fit_model_on_subset(model_func, train_data)
        val_score = evaluate_on_subset(result.x, val_data)
        cv_scores.append(val_score)
    
    return np.mean(cv_scores), np.std(cv_scores)
```

## Заключение и итоговые рекомендации

### Успехи текущего подхода:
1. **Одномерная регрессия:** Отличные результаты (R² = 98.87%)
2. **Алгоритм BFGS:** Быстрая и надежная сходимость
3. **Функция Хубера:** Показывает преимущества над L2-нормой

### Области для улучшения:
1. **Многомерная модель:** Требует пересмотра архитектуры
2. **Множественные запуски:** Для избежания локальных минимумов
3. **Валидация модели:** Cross-validation и анализ остатков

### Итоговый рейтинг методов:

| Метод | Точность | Скорость | Надежность | Общая оценка |
|-------|----------|----------|------------|---------------|
| **BFGS (текущий)** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | **9.3/10** |
| Trust Region | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | **9.0/10** |
| Levenberg-Marquardt | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | **8.7/10** |
| Градиентный спуск | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | **5.3/10** |
| Случайный поиск | ⭐ | ⭐ | ⭐ | **2.0/10** |

**Вывод:** Текущий подход с BFGS является одним из лучших для данной задачи, особенно для одномерного случая. Для многомерной задачи рекомендуется пересмотр архитектуры модели и применение дополнительных техник валидации.