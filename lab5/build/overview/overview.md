# Методы оптимизации: Теория и практическое применение

## Содержание
1. [Введение](#введение)
2. [Метод Нелдера-Мида](#метод-нелдера-мида)
3. [Метод Ньютона](#метод-ньютона)
4. [Метод BFGS](#метод-bfgs)
5. [Метод штрафных функций](#метод-штрафных-функций)
6. [Сравнение методов](#сравнение-методов)

## Введение

Методы оптимизации используются для поиска минимума или максимума функций. В зависимости от свойств целевой функции и наличия ограничений применяются различные подходы.

## Метод Нелдера-Мида

### Описание
Метод Нелдера-Мида (симплекс-метод) - это алгоритм прямого поиска, который не требует вычисления производных. Он использует геометрическую фигуру, называемую симплексом.

### Принцип работы
1. **Инициализация**: Создается симплекс из n+1 точек в n-мерном пространстве
2. **Оценка**: Вычисляются значения функции во всех вершинах симплекса
3. **Трансформации**: Применяются операции отражения, растяжения, сжатия и редукции
4. **Итерации**: Процесс повторяется до достижения сходимости

### Операции симплекса
- **Отражение**: Отражение худшей точки относительно центроида остальных точек
- **Растяжение**: Если отраженная точка лучше всех остальных, пробуем растянуть еще дальше
- **Сжатие**: Если отражение не улучшило результат, сжимаем симплекс
- **Редукция**: В крайнем случае уменьшаем весь симплекс к лучшей точке

### Преимущества и недостатки
**Преимущества:**
- Не требует вычисления производных
- Робустен к шуму в функции
- Прост в реализации

**Недостатки:**
- Медленная сходимость для больших размерностей
- Может застревать в локальных минимумах
- Неэффективен для функций с большим числом переменных (N > 10-20)

```python
# Пример применения
from scipy.optimize import minimize

def objective(x):
    return (x[0] - 1)**2 + (x[1] - 2)**2

result = minimize(objective, [0, 0], method='Nelder-Mead')
```

## Метод Ньютона

### Описание
Метод Ньютона использует информацию второго порядка (матрицу Гессе) для быстрой сходимости к минимуму.

### Математическая основа
Итерационная формула:
```
x_{k+1} = x_k - H^{-1}(x_k) * ∇f(x_k)
```

где:
- `H(x_k)` - матрица Гессе (вторые производные)
- `∇f(x_k)` - градиент функции

### Принцип работы
1. **Вычисление градиента**: ∇f(x_k)
2. **Вычисление Гессиана**: H(x_k)
3. **Решение системы**: H(x_k) * p_k = -∇f(x_k)
4. **Обновление**: x_{k+1} = x_k + p_k

### Преимущества и недостатки
**Преимущества:**
- Квадратичная скорость сходимости вблизи минимума
- Точен для квадратичных функций

**Недостатки:**
- Требует вычисления и обращения матрицы Гессе
- Может расходиться, если начальная точка далека от минимума
- Вычислительно затратен для больших размерностей

```python
# Пример с градиентом и Гессианом
def gradient(x):
    return np.array([2*(x[0]-1), 2*(x[1]-2)])

def hessian(x):
    return np.array([[2, 0], [0, 2]])

result = minimize(objective, [0, 0], method='Newton-CG', 
                 jac=gradient, hess=hessian)
```

## Метод BFGS

### Описание
BFGS (Broyden-Fletcher-Goldfarb-Shanno) - квазиньютоновский метод, который аппроксимирует матрицу Гессе, используя только информацию о градиенте.

### Принцип работы
1. **Инициализация**: Начинаем с единичной матрицы как приближения H^{-1}
2. **Поиск направления**: p_k = -B_k * ∇f(x_k)
3. **Линейный поиск**: Находим оптимальный шаг α_k
4. **Обновление**: Корректируем приближение обратного Гессиана

### Формула обновления BFGS
```
B_{k+1} = B_k + (y_k * y_k^T)/(y_k^T * s_k) - (B_k * s_k * s_k^T * B_k)/(s_k^T * B_k * s_k)
```

где:
- `s_k = x_{k+1} - x_k`
- `y_k = ∇f(x_{k+1}) - ∇f(x_k)`

### Преимущества и недостатки
**Преимущества:**
- Суперлинейная сходимость
- Не требует вычисления вторых производных
- Хорошо работает для большинства гладких функций
- Эффективен по памяти (O(n²))

**Недостатки:**
- Требует точного линейного поиска
- Память растет как O(n²)
- Может быть неустойчив для невыпуклых функций

```python
# BFGS требует только градиент
result = minimize(objective, [0, 0], method='BFGS', jac=gradient)
```

## Метод штрафных функций

### Описание
Метод штрафных функций преобразует задачу оптимизации с ограничениями в последовательность задач без ограничений.

### Математическая формулировка
Исходная задача:
```
minimize f(x)
subject to: g_i(x) ≤ 0, i = 1,...,m
           h_j(x) = 0, j = 1,...,p
```

Штрафная функция:
```
P(x, r) = f(x) + r * Σ[max(0, g_i(x))]² + r * Σ[h_j(x)]²
```

### Алгоритм
1. **Начальный параметр**: r_0 > 0
2. **Решение**: Минимизируем P(x, r_k) → x_k
3. **Увеличение штрафа**: r_{k+1} = γ * r_k, где γ > 1
4. **Повторение**: До сходимости

### Типы штрафных функций

#### Внешние штрафы
Штрафуют нарушение ограничений:
```python
def external_penalty(x, r):
    penalty = 0
    for constraint in constraints:
        violation = max(0, -constraint(x))  # для g(x) ≤ 0
        penalty += r * violation**2
    return objective(x) + penalty
```

#### Внутренние штрафы (барьерные)
Не позволяют покидать допустимую область:
```python
def barrier_penalty(x, r):
    penalty = 0
    for constraint in constraints:
        if constraint(x) <= 0:
            return float('inf')  # вне допустимой области
        penalty += -r * np.log(constraint(x))
    return objective(x) + penalty
```

### Преимущества и недостатки
**Преимущества:**
- Простота реализации
- Превращает задачу с ограничениями в задачу без ограничений
- Гибкость в выборе штрафных функций

**Недостатки:**
- Плохая обусловленность при больших r
- Медленная сходимость
- Необходимость настройки параметра штрафа

## Сравнение методов

### Таблица сравнения

| Характеристика | Нелдер-Мид | Ньютон | BFGS | Штрафные функции |
|----------------|------------|--------|------|------------------|
| **Производные** | Не нужны | 1-я и 2-я | Только 1-я | Зависит от базового метода |
| **Сходимость** | Линейная | Квадратичная | Суперлинейная | Зависит от параметров |
| **Память** | O(n) | O(n²) | O(n²) | O(n) + базовый метод |
| **Размерность** | Плохо для n>20 | Хорошо | Отлично | Универсально |
| **Ограничения** | Нет | Нет | Нет | Да |
| **Робастность** | Высокая | Низкая | Средняя | Средняя |

### Рекомендации по выбору

#### Нелдер-Мид
- Малые размерности (n ≤ 10)
- Зашумленные функции
- Недоступность производных
- Разведочный анализ

#### Ньютон
- Квадратичные или близкие к квадратичным функции
- Доступны точные вторые производные
- Высокие требования к точности

#### BFGS
- Гладкие функции средней и большой размерности
- Доступен градиент
- Баланс между скоростью и надежностью
- **Рекомендуется как основной метод**

#### Штрафные функции
- Задачи с ограничениями
- Простые ограничения типа неравенств
- Когда другие методы недоступны

### Практические советы

1. **Начальное приближение**: Выбирайте близко к ожидаемому решению
2. **Масштабирование**: Нормализуйте переменные для улучшения обусловленности
3. **Проверка градиента**: Используйте численную проверку аналитического градиента
4. **Мониторинг сходимости**: Отслеживайте значения функции и норму градиента
5. **Гибридные подходы**: Комбинируйте методы (например, Нелдер-Мид + BFGS)

### Пример полного решения

```python
import numpy as np
from scipy.optimize import minimize

# Определение задачи
def objective(x):
    return (x[0] - 1)**2 + (x[1] - 2)**2

def constraint(x):
    return x[0]**2 + x[1]**2 - 4  # x² + y² ≤ 4

# Метод 1: Безусловная оптимизация
result_bfgs = minimize(objective, [0, 0], method='BFGS')

# Метод 2: С ограничениями
constraints = {'type': 'ineq', 'fun': lambda x: 4 - x[0]**2 - x[1]**2}
result_constrained = minimize(objective, [0, 0], method='SLSQP', 
                            constraints=constraints)

# Метод 3: Штрафные функции
def penalty_objective(x, r=100):
    penalty = r * max(0, x[0]**2 + x[1]**2 - 4)**2
    return objective(x) + penalty

result_penalty = minimize(penalty_objective, [0, 0], method='BFGS')
```

## Заключение

Выбор метода оптимизации зависит от:
- Свойств целевой функции (гладкость, выпуклость)
- Размерности задачи
- Доступности производных
- Наличия ограничений
- Требований к точности и скорости

Для большинства практических задач рекомендуется начинать с метода BFGS, а при наличии ограничений использовать SLSQP или современные методы внутренней точки.



# Барьерный метод и метод штрафных функций

## Содержание
1. [Введение](#введение)
2. [Общая постановка задачи](#общая-постановка-задачи)
3. [Метод штрафных функций](#метод-штрафных-функций)
4. [Барьерный метод](#барьерный-метод)
5. [Сравнение методов](#сравнение-методов)
6. [Практические аспекты](#практические-аспекты)
7. [Примеры реализации](#примеры-реализации)

## Введение

Барьерный метод и метод штрафных функций являются классическими подходами для решения задач оптимизации с ограничениями. Оба метода преобразуют исходную задачу с ограничениями в последовательность задач безусловной оптимизации.

**Основная идея**: Добавить к целевой функции дополнительные слагаемые, которые "наказывают" за нарушение ограничений или препятствуют выходу за пределы допустимой области.

## Общая постановка задачи

Рассмотрим задачу нелинейного программирования:

```
minimize f(x)
subject to: g_i(x) ≤ 0,  i = 1, 2, ..., m
           h_j(x) = 0,   j = 1, 2, ..., p
           x ∈ R^n
```

где:
- `f(x)` — целевая функция
- `g_i(x) ≤ 0` — ограничения-неравенства
- `h_j(x) = 0` — ограничения-равенства
- `x` — вектор переменных

**Допустимая область**: `S = {x : g_i(x) ≤ 0, h_j(x) = 0}`

## Метод штрафных функций

### Основная идея

Метод штрафных функций добавляет к целевой функции **штрафные слагаемые**, которые становятся большими при нарушении ограничений. Это позволяет точкам покидать допустимую область, но "наказывает" за это.

### Математическая формулировка

**Штрафная функция:**
```
P(x, r_k) = f(x) + r_k * Φ(x)
```

где:
- `r_k > 0` — параметр штрафа (увеличивается с каждой итерацией)
- `Φ(x)` — функция штрафа

### Типы штрафных функций

#### 1. Внешние штрафы (External Penalty)

Используются для ограничений-неравенств:

```
Φ(x) = Σ[max(0, g_i(x))]² + Σ[h_j(x)]²
```

**Свойства:**
- Позволяют начинать из недопустимых точек
- Штрафуют только нарушения ограничений
- Последовательность решений сходится к допустимой области извне

#### 2. Внутренние штрафы (Interior Penalty / Barrier Method)

Рассмотрим отдельно в следующем разделе.

### Алгоритм внешних штрафов

```
1. Выбрать начальную точку x₀ и r₀ > 0
2. Для k = 0, 1, 2, ...
   a) Решить задачу: minimize P(x, r_k) → x_k
   b) Проверить критерий остановки
   c) Увеличить штраф: r_{k+1} = β * r_k, где β > 1
3. Вернуть приближенное решение
```

### Пример реализации внешних штрафов

```python
def external_penalty_function(x, r, constraints):
    """Внешняя штрафная функция"""
    penalty = 0
    
    # Ограничения-неравенства g_i(x) ≤ 0
    for g in constraints['ineq']:
        violation = max(0, g(x))
        penalty += r * violation**2
    
    # Ограничения-равенства h_j(x) = 0
    for h in constraints['eq']:
        penalty += r * h(x)**2
    
    return objective(x) + penalty

def external_penalty_method(objective, constraints, x0):
    """Метод внешних штрафов"""
    r = 1.0  # Начальный параметр штрафа
    beta = 10.0  # Коэффициент увеличения
    max_iter = 10
    
    x = x0.copy()
    results = []
    
    for k in range(max_iter):
        # Определяем штрафную функцию для текущего r
        def penalty_obj(x):
            return external_penalty_function(x, r, constraints)
        
        # Решаем задачу безусловной оптимизации
        result = minimize(penalty_obj, x, method='BFGS')
        x = result.x
        
        results.append({
            'iteration': k,
            'penalty_param': r,
            'x': x.copy(),
            'objective': objective(x),
            'penalty_function': result.fun
        })
        
        # Увеличиваем параметр штрафа
        r *= beta
    
    return results
```

### Преимущества и недостатки внешних штрафов

**Преимущества:**
- Можно начинать из любой точки
- Простота реализации
- Гарантированная сходимость при выполнении условий регулярности

**Недостатки:**
- Плохая обусловленность при больших `r_k`
- Медленная сходимость
- Решение может не принадлежать допустимой области при конечном числе итераций

## Барьерный метод

### Основная идея

Барьерный метод (метод внутренних штрафов) добавляет к целевой функции **барьерные слагаемые**, которые стремятся к бесконечности при приближении к границе допустимой области. Это не позволяет точкам покидать допустимую область.

### Математическая формулировка

**Барьерная функция:**
```
B(x, μ_k) = f(x) + μ_k * Ψ(x)
```

где:
- `μ_k > 0` — барьерный параметр (уменьшается с каждой итерацией)
- `Ψ(x)` — барьерная функция

### Типы барьерных функций

#### 1. Логарифмический барьер

Для ограничений `g_i(x) ≤ 0`:

```
Ψ(x) = -Σ ln(-g_i(x))
```

**Условие применимости:** `g_i(x) < 0` для всех i (строго внутри допустимой области)

#### 2. Обратный барьер

```
Ψ(x) = Σ 1/(-g_i(x))
```

#### 3. Степенной барьер

```
Ψ(x) = Σ (-g_i(x))^(-α), где α > 0
```

### Алгоритм барьерного метода

```
1. Найти начальную точку x₀ строго внутри допустимой области
2. Выбрать μ₀ > 0
3. Для k = 0, 1, 2, ...
   a) Решить задачу: minimize B(x, μ_k) → x_k
   b) Проверить критерий остановки
   c) Уменьшить барьер: μ_{k+1} = μ_k / β, где β > 1
4. Вернуть приближенное решение
```

### Пример реализации барьерного метода

```python
def logarithmic_barrier(x, mu, constraints):
    """Логарифмический барьер"""
    barrier = 0
    
    for g in constraints:
        constraint_value = g(x)
        if constraint_value >= 0:
            return np.inf  # Вне допустимой области
        barrier += -mu * np.log(-constraint_value)
    
    return objective(x) + barrier

def find_feasible_start(constraints, bounds=(-5, 5), max_tries=1000):
    """Поиск начальной допустимой точки"""
    for _ in range(max_tries):
        x = np.random.uniform(bounds[0], bounds[1], n_vars)
        if all(g(x) < 0 for g in constraints):
            return x
    raise ValueError("Не удалось найти допустимую начальную точку")

def barrier_method(objective, constraints, n_vars):
    """Барьерный метод"""
    # Находим допустимую начальную точку
    x0 = find_feasible_start(constraints)
    
    mu = 1.0  # Начальный барьерный параметр
    reduction_factor = 0.1  # Коэффициент уменьшения
    max_iter = 10
    
    x = x0.copy()
    results = []
    
    for k in range(max_iter):
        # Определяем барьерную функцию для текущего μ
        def barrier_obj(x):
            return logarithmic_barrier(x, mu, constraints)
        
        # Решаем задачу безусловной оптимизации
        result = minimize(barrier_obj, x, method='BFGS')
        
        if not result.success:
            print(f"Оптимизация не сошлась на итерации {k}")
            break
            
        x = result.x
        
        # Проверяем, что остаемся в допустимой области
        if not all(g(x) < 0 for g in constraints):
            print(f"Покинули допустимую область на итерации {k}")
            break
        
        results.append({
            'iteration': k,
            'barrier_param': mu,
            'x': x.copy(),
            'objective': objective(x),
            'barrier_function': result.fun,
            'constraint_values': [g(x) for g in constraints]
        })
        
        # Уменьшаем барьерный параметр
        mu *= reduction_factor
    
    return results
```

### Преимущества и недостатки барьерного метода

**Преимущества:**
- Все промежуточные точки допустимы
- Хорошая численная устойчивость
- Эффективен для выпуклых задач

**Недостатки:**
- Требует строго допустимую начальную точку
- Применим только к ограничениям-неравенствам
- Может быть сложно найти начальную точку

## Сравнение методов

### Таблица сравнения

| Характеристика | Внешние штрафы | Барьерный метод |
|----------------|----------------|-----------------|
| **Начальная точка** | Любая | Строго допустимая |
| **Промежуточные точки** | Могут быть недопустимыми | Всегда допустимы |
| **Направление сходимости** | Извне к границе | Изнутри к границе |
| **Параметр** | Увеличивается (r_k ↗) | Уменьшается (μ_k ↘) |
| **Применимость** | Неравенства + равенства | Только неравенства |
| **Обусловленность** | Ухудшается | Остается приемлемой |
| **Поиск начальной точки** | Простой | Может быть сложным |

### Графическое сравнение

```
Внешние штрафы:          Барьерный метод:
                        
   ×  ×  ×                   
     ↘ ↘ ↘              ○ → ○ → ○ → ●
×  ×  ● ←── Оптимум     ↑         ↗
     ↗ ↗ ↗              ○ ← ○ ← ○
   ×  ×  ×              
                        
Сходимость извне        Сходимость изнутри
```

### Теоретические свойства

#### Сходимость внешних штрафов

**Теорема**: Пусть `x*` — решение исходной задачи, `x_k` — решение штрафной задачи с параметром `r_k`. Если `r_k → ∞`, то:

```
lim_{k→∞} x_k = x*
lim_{k→∞} f(x_k) = f(x*)
```

#### Сходимость барьерного метода

**Теорема**: Пусть задача выпукла и выполнены условия регулярности. Если `μ_k → 0`, то:

```
lim_{k→∞} x_k = x*
f(x_k) ≥ f(x*) для всех k (монотонность)
```

## Практические аспекты

### Выбор параметров

#### Для внешних штрафов:
- **Начальный штраф**: `r_0 = 1` или `r_0 = 10`
- **Коэффициент роста**: `β = 10` (типично от 5 до 100)
- **Критерий остановки**: `||∇P(x_k, r_k)|| < ε` или `max{g_i(x_k), |h_j(x_k)|} < ε`

#### Для барьерного метода:
- **Начальный барьер**: `μ_0 = 1` или `μ_0 = 0.1`
- **Коэффициент уменьшения**: `γ = 0.1` (типично от 0.01 до 0.5)
- **Критерий остановки**: `μ_k * m < ε`, где m — число ограничений

### Численные проблемы и их решение

#### 1. Плохая обусловленность (внешние штрафы)

**Проблема**: При больших `r_k` матрица Гессе становится плохо обусловленной.

**Решение**:
- Использовать более аккуратные методы оптимизации
- Адаптивный выбор параметра штрафа
- Переход к методам множителей Лагранжа

#### 2. Поиск допустимой начальной точки (барьерный метод)

**Проблема**: Сложно найти точку строго внутри допустимой области.

**Решения**:
```python
# Метод 1: Случайный поиск
def random_feasible_search(constraints, bounds, max_tries=10000):
    for _ in range(max_tries):
        x = np.random.uniform(bounds[0], bounds[1], len(bounds[0]))
        if all(g(x) < -1e-6 for g in constraints):  # Строго внутри
            return x
    return None

# Метод 2: Решение вспомогательной задачи
def find_feasible_via_auxiliary(constraints):
    # minimize t
    # subject to: g_i(x) ≤ t, i = 1,...,m
    #            t ≤ 0
    pass

# Метод 3: Сжатие области
def shrink_constraints(constraints, shrink_factor=0.9):
    return [lambda x, g=g: g(x) + shrink_factor * abs(g(x)) 
            for g in constraints]
```

#### 3. Выбор последовательности параметров

**Адаптивная стратегия**:
```python
def adaptive_parameter_update(current_param, constraint_violations, 
                            method='penalty'):
    if method == 'penalty':
        # Увеличиваем штраф, если нарушения большие
        max_violation = max(constraint_violations)
        if max_violation > 1e-3:
            return current_param * 10
        else:
            return current_param * 2
    
    elif method == 'barrier':
        # Уменьшаем барьер, если близко к оптимуму
        min_distance = min(abs(v) for v in constraint_violations)
        if min_distance > 1e-2:
            return current_param * 0.5
        else:
            return current_param * 0.1
```

### Гибридные подходы

#### 1. Двухфазный метод
```python
def two_phase_method(objective, constraints, x0):
    # Фаза 1: Внешние штрафы для поиска допустимой области
    penalty_result = external_penalty_method(objective, constraints, x0)
    
    # Фаза 2: Барьерный метод для точного решения
    if is_feasible(penalty_result.x, constraints):
        barrier_result = barrier_method(objective, constraints, 
                                      penalty_result.x)
        return barrier_result
    else:
        return penalty_result
```

#### 2. Метод увеличенного Лагранжиана

Комбинирует штрафы с множителями Лагранжа:

```
L(x, λ, r) = f(x) + Σλ_i * g_i(x) + (r/2) * Σ[max(0, g_i(x))]²
```

## Примеры реализации

### Полный пример для конкретной задачи

```python
import numpy as np
from scipy.optimize import minimize
import matplotlib.pyplot as plt

# Целевая функция: (x²)/10 + (y²)/3 - (xy)/5
def objective(x):
    return x[0]**2/10 + x[1]**2/3 - x[0]*x[1]/5

# Ограничения: g_i(x) ≤ 0
def constraint1(x):
    return x[0]**2/10 + x[1]**2/3 - x[0]*x[1]/5 - 6

def constraint2(x):
    return x[0]**2/3 + x[1]**2/104 + x[0]*x[1]/5 - 3

constraints = [constraint1, constraint2]

# Внешние штрафы
def penalty_function(x, r):
    penalty = 0
    for g in constraints:
        penalty += r * max(0, g(x))**2
    return objective(x) + penalty

# Барьерная функция
def barrier_function(x, mu):
    barrier = 0
    for g in constraints:
        if g(x) >= 0:
            return np.inf
        barrier += -mu * np.log(-g(x))
    return objective(x) + barrier

# Решение внешними штрафами
def solve_penalty():
    x = np.array([0.0, 0.0])
    r_values = [1, 10, 100, 1000]
    
    for r in r_values:
        def penalty_obj(x):
            return penalty_function(x, r)
        
        result = minimize(penalty_obj, x, method='BFGS')
        x = result.x
        
        print(f"Штраф r={r}: x={x}, f={objective(x):.6f}")
        print(f"  Ограничения: {[g(x) for g in constraints]}")

# Решение барьерным методом
def solve_barrier():
    # Поиск допустимой начальной точки
    x = None
    for _ in range(1000):
        candidate = np.random.uniform(-2, 2, 2)
        if all(g(candidate) < 0 for g in constraints):
            x = candidate
            break
    
    if x is None:
        print("Не найдена допустимая начальная точка")
        return
    
    mu_values = [1, 0.1, 0.01, 0.001]
    
    for mu in mu_values:
        def barrier_obj(x):
            return barrier_function(x, mu)
        
        result = minimize(barrier_obj, x, method='BFGS')
        if result.success:
            x = result.x
            print(f"Барьер μ={mu}: x={x}, f={objective(x):.6f}")
            print(f"  Ограничения: {[g(x) for g in constraints]}")

# Запуск решения
print("=== МЕТОД ВНЕШНИХ ШТРАФОВ ===")
solve_penalty()

print("\n=== БАРЬЕРНЫЙ МЕТОД ===")
solve_barrier()
```

### Визуализация результатов

```python
def visualize_methods():
    # Создаем сетку для визуализации
    x = np.linspace(-4, 4, 200)
    y = np.linspace(-4, 4, 200)
    X, Y = np.meshgrid(x, y)
    
    # Вычисляем значения функций
    Z = X**2/10 + Y**2/3 - X*Y/5
    C1 = X**2/10 + Y**2/3 - X*Y/5 - 6
    C2 = X**2/3 + Y**2/104 + X*Y/5 - 3
    
    # Допустимая область
    feasible = (C1 <= 0) & (C2 <= 0)
    
    plt.figure(figsize=(15, 5))
    
    # График 1: Общий вид
    plt.subplot(1, 3, 1)
    plt.contour(X, Y, Z, levels=20, alpha=0.6)
    plt.contour(X, Y, C1, levels=[0], colors='red', linewidths=2)
    plt.contour(X, Y, C2, levels=[0], colors='green', linewidths=2)
    plt.contourf(X, Y, feasible, levels=[0.5, 1.5], 
                colors=['lightblue'], alpha=0.3)
    plt.title('Задача оптимизации')
    plt.xlabel('x₁')
    plt.ylabel('x₂')
    
    # График 2: Штрафная функция
    plt.subplot(1, 3, 2)
    r = 100
    Z_penalty = Z + r * (np.maximum(0, C1)**2 + np.maximum(0, C2)**2)
    plt.contour(X, Y, Z_penalty, levels=20)
    plt.title(f'Штрафная функция (r={r})')
    plt.xlabel('x₁')
    plt.ylabel('x₂')
    
    # График 3: Барьерная функция (только для допустимой области)
    plt.subplot(1, 3, 3)
    mu = 0.1
    Z_barrier = np.where(feasible, 
                        Z - mu*(np.log(-C1) + np.log(-C2)), 
                        np.inf)
    
    # Ограничиваем значения для визуализации
    Z_barrier = np.where(Z_barrier > 10, 10, Z_barrier)
    
    plt.contour(X, Y, Z_barrier, levels=20)
    plt.title(f'Барьерная функция (μ={mu})')
    plt.xlabel('x₁')
    plt.ylabel('x₂')
    
    plt.tight_layout()
    plt.show()

visualize_methods()
```

## Заключение

### Рекомендации по применению

1. **Используйте внешние штрафы, если:**
   - Неизвестна допустимая начальная точка
   - Есть ограничения-равенства
   - Нужна гарантированная сходимость

2. **Используйте барьерный метод, если:**
   - Легко найти допустимую начальную точку
   - Все ограничения — неравенства
   - Важна допустимость промежуточных решений

3. **Современные альтернативы:**
   - Sequential Quadratic Programming (SQP)
   - Interior Point Methods
   - Augmented Lagrangian Methods

### Развитие методов

Барьерный метод и метод штрафных функций заложили основу для современных методов внутренней точки, которые сейчас широко используются в коммерческих решателях оптимизации.

**Ключевые улучшения:**
- Primal-dual interior point methods
- Trust region approaches
- Adaptive parameter strategies
- Filter methods for global convergence